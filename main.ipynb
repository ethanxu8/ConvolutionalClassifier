{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building A Convolutional Image Classifier With Keras and Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project revolves around useing modern deep-learning networks to build an image classifier with Keras. We will design our own custom convnet with reusable blocks and perform visual feature extraction. We will also use transfer learning to boost our model and utilzie data augmentation to extend our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let's break down the theory behind our project so we get a better understanding. \n",
    "\n",
    "The goal of our project is to design a neural network which can \"understand\" a natural image well-enough to solve the same kinds of problems the human visual system can solve. \n",
    "\n",
    "There are many neural networks (eg. RNNs, GNNs, CNNs), all utilized for different purposes and applications in machine learning. For example, Recurrent Neural Networks (RNNs) are great for text based classification tasks. This can include objectives like sentiment analysis. The neural networks that are best for image classification are called convolutional neural networks (CNN or convnet). \n",
    "\n",
    "A CNN consists of two parts: a convolutional base and a dense head. \n",
    "\n",
    "The base is used to extract the features from an image. WHat does this mean? Each convolutional layer applies filters (small matrices) that detect specific patterns in different part of the image. These filters help break down the image into different levels of abstraction. \n",
    "- First layers detect basic features (edges, corners, textures). \n",
    "- Middle layers detect more complex structures (shapes, object). \n",
    "- Deeper layers recognize high-level features (eg. faces, cats, cars).\n",
    "\n",
    "Each layer transforms the image into multiple feature maps, which are \"filtered versions\" of the image, highlighting different aspects. By gradually learning from low-level details to high-level concepts, the CNN builds an abstract understanding of the image inputted allowing the classification to become easier. \n",
    "\n",
    "The head now recieves meaninfgul structured information from the base instead of raw data/pixels. This allows it to make an educated guess. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, during training, we want our network to learn two things. \n",
    "1. which features to extract from an image \n",
    "2. which class goes with what features\n",
    "\n",
    "CNNs are rarely trained from scratch and a more common approach is to reuse the base of a pretrained model. To the pretrained base, we can then attach an untrained head. In other wrods, we reuse thepart of a network that has already learned to extract features adn attach it to some fresh layers to learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough talking, let's get coding! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.2.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/envs/cnnproj/lib/python3.8/site-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/envs/cnnproj/lib/python3.8/site-packages (from kagglehub) (2.32.3)\n",
      "Collecting tqdm (from kagglehub)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/cnnproj/lib/python3.8/site-packages (from requests->kagglehub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/cnnproj/lib/python3.8/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/cnnproj/lib/python3.8/site-packages (from requests->kagglehub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/cnnproj/lib/python3.8/site-packages (from requests->kagglehub) (2024.8.30)\n",
      "Downloading kagglehub-0.2.9-py3-none-any.whl (39 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, kagglehub\n",
      "Successfully installed kagglehub-0.2.9 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/cnnproj/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.6)\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/ryanholbrook/car-or-truck?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77.7M/77.7M [00:04<00:00, 18.0MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting model files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5117 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-29 19:37:28.945972: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5051 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "%pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed=31415):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "set_seed()\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('image', cmap='magma')\n",
    "warnings.filterwarnings(\"ignore\") # to clean up output cells\n",
    "\n",
    "# Download the dataset using KaggleHub API\n",
    "path = kagglehub.dataset_download(\"ryanholbrook/car-or-truck\")\n",
    "\n",
    "# Load training and validation sets\n",
    "ds_train_ = image_dataset_from_directory(\n",
    "    os.path.join(path, 'train'),\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    image_size=[128, 128],\n",
    "    interpolation='nearest',\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")\n",
    "ds_valid_ = image_dataset_from_directory(\n",
    "    os.path.join(path, 'valid'),\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    image_size=[128, 128],\n",
    "    interpolation='nearest',\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Data Pipeline\n",
    "def convert_to_float(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    return image, label\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "ds_train = (\n",
    "    ds_train_\n",
    "    .map(convert_to_float)\n",
    "    .cache()\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n",
    "ds_valid = (\n",
    "    ds_valid_\n",
    "    .map(convert_to_float)\n",
    "    .cache()\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Define Pretrained Base "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most commonly used dataset for pretraining is ImageNet, a large dataset of many kind of natural images. Keras includes a variety models pretrained on ImageNet in its applications module. The pretrained model we'll use is called VGG16. \n",
    "\n",
    "Some information abotu VGG16: \n",
    "\n",
    "Architecture:\n",
    "\n",
    "VGG16 consists of 16 layers in total, which include:\n",
    "- 13 convolutional layers: These layers apply filters to the input image to detect features like edges, textures, and patterns.\n",
    "- 3 fully connected layers: After extracting features through convolution, these layers help the network make decisions or classifications based on those features.\n",
    "It uses a relatively simple and uniform structure where each convolutional layer uses 3x3 filters and 2x2 max-pooling layers. This helps the model focus on learning spatial hierarchies of features in images.\n",
    "\n",
    "Pretrained on ImageNet:\n",
    "- VGG16 is commonly used as a pre-trained model that has been trained on a large dataset called ImageNet. ImageNet consists of millions of labeled images across 1000 different categories (e.g., animals, objects, scenes).\n",
    "Because of this, VGG16 has already learned to extract general features from images (e.g., edges, textures, shapes) and can be used for transfer learning, where you fine-tune it for a specific task (such as classifying new categories of images).\n",
    "\n",
    "Why use VGG16?\n",
    "- Transfer Learning: Instead of training a deep neural network from scratch (which can be computationally expensive), you can use VGG16 pretrained on ImageNet. This approach allows you to leverage the features the model has already learned, and you can fine-tune it to your own dataset. This is often faster and requires less data.\n",
    "Performance: VGG16 has proven to be a very effective model for image classification and is used in many research and real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 3s 0us/step\n",
      "58900480/58889256 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Load the VGG16 model pre-trained on ImageNet\n",
    "pretrained_base = VGG16(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(128, 128, 3)\n",
    ")\n",
    "pretrained_base.trainable = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cnnproj)",
   "language": "python",
   "name": "cnnproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
